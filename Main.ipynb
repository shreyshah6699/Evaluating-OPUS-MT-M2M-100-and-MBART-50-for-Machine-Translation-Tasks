{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jA-jN-cU4DSJ"
   },
   "source": [
    "# 1. Data preparation (5points)\n",
    "## 1) Data download and preprocessing: Download the Flores200 dataset, which is a benchmark data for machine translation between English and low-resource languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sMWnT-pcAmU3"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"Muennighoff/flores200\", 'eng_Latn-fra_Latn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OgUlkAqNAmSa"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "selected_data = dataset['dev']\n",
    "\n",
    "# Set a fixed seed for reproducibility\n",
    "random.seed(123)\n",
    "\n",
    "# Randomly choose 100 distinct items\n",
    "rndm_indices = random.sample(range(len(selected_data)), 100)\n",
    "rndm_samples = selected_data.select(rndm_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kljboOkR4DSO"
   },
   "source": [
    "## 2. Machine Translation with Seq2Seq model (65 points, 20 points for each model, 5 points for data statistics)\n",
    "Please use the following models to perform machine translation on the data you prepared.\n",
    "• Feel free to directly utilize the existing implementations on Hugging Face.\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"Muennighoff/flores200\")\n",
    "2\n",
    "• There is no specific requirement for the parameter settings. You are encouraged to try\n",
    "and test different settings and report the results. For other settings that are not specified\n",
    "here, you have the flexibility to select.\n",
    "• If you want to perform the translation more efficiently with the following models, you can\n",
    "consider using CTranslate2. Its Github repository is available here. The documentation of\n",
    "the package is available here. M2M-100 and MBART-50 are under Fairseq.\n",
    "• During the implementation, for each model, you will need to specify the source language\n",
    "and target language or select the specific model for your source and target languages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 1 (OPUS-MT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3qwNnbAOAmPI"
   },
   "outputs": [],
   "source": [
    "# Import necessary modules from the transformers library\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Initialize the tokenizer for the English to French translation model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "\n",
    "# Initialize the Seq2Seq model for the English to French translation\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "e-EXL3IcCp-G"
   },
   "outputs": [],
   "source": [
    "def translate_text_1(input_text):\n",
    "    # Tokenize input\n",
    "    tokenized_text = tokenizer(input_text, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Generate translation\n",
    "    translation_output = model.generate(**tokenized_text)\n",
    "\n",
    "    # Decode and return translation\n",
    "    return tokenizer.decode(translation_output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Yu6h-Bu-CvcR"
   },
   "outputs": [],
   "source": [
    "english=[]\n",
    "# Use list comprehension to translate all English texts to French\n",
    "translated_texts_1= [translate_text_1(item['sentence_eng_Latn']) for item in rndm_samples]\n",
    "english = [item['sentence_eng_Latn'] for item in rndm_samples]\n",
    "\n",
    "# Extract all original French texts\n",
    "original_texts = [item['sentence_fra_Latn'] for item in rndm_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovxI9esMH-w6"
   },
   "source": [
    "# MODEL 2 (M2M-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "r_H89S4SH-jG"
   },
   "outputs": [],
   "source": [
    "# Import necessary modules from the transformers library\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Initialize the tokenizer for the m2m100_418M model\n",
    "m2m_tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# Initialize the Seq2Seq model for the m2m100_418M model\n",
    "m2m_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# Set the source language for the tokenizer\n",
    "m2m_tokenizer.src_lang = \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oNk6W4p_H-dY"
   },
   "outputs": [],
   "source": [
    "def translate_text_2(model_name, input_text):\n",
    "    # Tokenize the input text for translation\n",
    "    encoded_text = m2m_tokenizer(input_text, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Generate the translation output\n",
    "    translation_output = m2m_model.generate(**encoded_text, forced_bos_token_id=m2m_tokenizer.get_lang_id(\"fr\"))\n",
    "\n",
    "    # Return the decoded translated text\n",
    "    return m2m_tokenizer.batch_decode(translation_output, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dk9qw5J9H-Xe"
   },
   "outputs": [],
   "source": [
    "# Use list comprehension to translate all English texts to French\n",
    "translated_texts_2 = [translate_text_2(\"facebook/m2m100_418M\",\\\n",
    "                                       item['sentence_eng_Latn']) for item in rndm_samples]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVBFylV5H-Nt"
   },
   "source": [
    "# Model 3 (MBART-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-J1_oWtSH-LH"
   },
   "outputs": [],
   "source": [
    "# Import necessary modules from the transformers library\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Initialize the tokenizer for the mBART-50 model\n",
    "mbart_tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "\n",
    "# Initialize the Seq2Seq model for the mBART-50 model\n",
    "mbart_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "\n",
    "# Set the source language for the tokenizer\n",
    "mbart_tokenizer.src_lang = \"en_XX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "scMJoZv74DST"
   },
   "outputs": [],
   "source": [
    "def translate_text_3(input_text):\n",
    "    # Tokenize the input text for translation\n",
    "    encoded_text = mbart_tokenizer(input_text, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Generate the translation output\n",
    "    translation_output = mbart_model.generate(**encoded_text, forced_bos_token_id=mbart_tokenizer.lang_code_to_id[\"fr_XX\"])\n",
    "\n",
    "    # Return the decoded translated text\n",
    "    return mbart_tokenizer.batch_decode(translation_output, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dCTMr_df4DST"
   },
   "outputs": [],
   "source": [
    "# Initialize list for storing translations\n",
    "translated_texts_3 = []\n",
    "\n",
    "# Translate each text in the random samples\n",
    "for item in rndm_samples:\n",
    "    # Extract English and original French text\n",
    "    eng_text = item['sentence_eng_Latn']\n",
    "\n",
    "\n",
    "    # Translate English text to French\n",
    "    trans_french = translate_text_3(eng_text)\n",
    "\n",
    "    # Append translated French text to the list\n",
    "    translated_texts_3.append(trans_french)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZ2NjfA-4DST"
   },
   "source": [
    "# Data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "f2B_Jxq04DSU",
    "outputId": "fbfd3310-9242-4b75-c0b1-5c853d05f4d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Sentences: \n",
      "Min Length = 12, \n",
      "Average Length = 31.48,       \n",
      "Max Length = 69, \n",
      "Median Length = 31.0, \n",
      "Standard Deviation = 9.701067316058943,       \n",
      "Variance = 94.11070707070706, \n",
      "Mode = 31\n",
      "\n",
      "\n",
      "French Sentences: \n",
      "Min Length = 13, \n",
      "Average Length = 39.27,      \n",
      "Max Length = 72, \n",
      "Median Length = 38.0,       \n",
      "Standard Deviation = 11.922184566952687, \n",
      "Variance = 142.13848484848484,       \n",
      "Mode = 33\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "# Calculate lengths of English and French sentences\n",
    "eng_lengths = [len(mbart_tokenizer(item['sentence_eng_Latn']).input_ids) for item in rndm_samples]\n",
    "french_lengths = [len(mbart_tokenizer(item['sentence_fra_Latn']).input_ids) for item in rndm_samples]\n",
    "\n",
    "# Compute basic statistics for English sentences\n",
    "min_length_eng = min(eng_lengths)\n",
    "avg_length_eng = sum(eng_lengths) / len(eng_lengths)\n",
    "max_length_eng = max(eng_lengths)\n",
    "median_length_eng = statistics.median(eng_lengths)\n",
    "std_dev_length_eng = statistics.stdev(eng_lengths)\n",
    "variance_length_eng = statistics.variance(eng_lengths)\n",
    "mode_length_eng = statistics.mode(eng_lengths)\n",
    "\n",
    "# Compute basic statistics for French sentences\n",
    "min_length_french = min(french_lengths)\n",
    "avg_length_french = sum(french_lengths) / len(french_lengths)\n",
    "max_length_french = max(french_lengths)\n",
    "median_length_french = statistics.median(french_lengths)\n",
    "std_dev_length_french = statistics.stdev(french_lengths)\n",
    "variance_length_french = statistics.variance(french_lengths)\n",
    "mode_length_french = statistics.mode(french_lengths)\n",
    "\n",
    "print(f\"English Sentences: \\nMin Length = {min_length_eng}, \\nAverage Length = {avg_length_eng}, \\\n",
    "      \\nMax Length = {max_length_eng}, \\nMedian Length = {median_length_eng}, \\nStandard Deviation = {std_dev_length_eng}, \\\n",
    "      \\nVariance = {variance_length_eng}, \\nMode = {mode_length_eng}\\n\\n\")\n",
    "print(f\"French Sentences: \\nMin Length = {min_length_french}, \\nAverage Length = {avg_length_french},\\\n",
    "      \\nMax Length = {max_length_french}, \\nMedian Length = {median_length_french}, \\\n",
    "      \\nStandard Deviation = {std_dev_length_french}, \\nVariance = {variance_length_french}, \\\n",
    "      \\nMode = {mode_length_french}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqGTLoDz4DSV"
   },
   "source": [
    "## 3. Results analysis and evaluation (30 points, 10 points for each sub-question)\n",
    "### 1) Please use the BLUE score to evaluate the performance of the models. You can find the details and examples about how to get BLUE scores in this link.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "id": "5v6rhplJHx18",
    "outputId": "c54ce5b8-c129-4bc0-d447-718b38bf6afa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score for Translation Model:\n",
      "{'bleu': 0.4382318045614185, 'precisions': [0.703788748564868, 0.4986072423398329, 0.38126813095731454, 0.29442282749675747], 'brevity_penalty': 0.9836784826124793, 'length_ratio': 0.9838102409638554, 'translation_length': 2613, 'reference_length': 2656}\n",
      "\n",
      "BLEU Score for Translation Model:\n",
      "{'bleu': 0.38514913523032057, 'precisions': [0.6721439749608764, 0.4531758957654723, 0.3331918505942275, 0.25354609929078015], 'brevity_penalty': 0.9616318146086646, 'length_ratio': 0.9623493975903614, 'translation_length': 2556, 'reference_length': 2656}\n",
      "\n",
      "BLEU Score for Translation Model:\n",
      "{'bleu': 0.3961594263710492, 'precisions': [0.6709973251815056, 0.4604688120778705, 0.33802234174596607, 0.2503236944324558], 'brevity_penalty': 0.9852079334065056, 'length_ratio': 0.985316265060241, 'translation_length': 2617, 'reference_length': 2656}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the evaluation module\n",
    "import evaluate\n",
    "\n",
    "# Initialize the BLEU metric\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "\n",
    "# Compute the BLEU score for OPUS-MT model\n",
    "bleu_score = bleu_metric.compute(predictions=translated_texts_1, references=original_texts)\n",
    "\n",
    "# Print the BLEU score for OPUS-MT model\n",
    "print(f\"BLEU Score for Translation Model:\\n{bleu_score}\\n\")\n",
    "\n",
    "# Compute the BLEU score for M2M-100 texts\n",
    "bleu_score = bleu_metric.compute(predictions=translated_texts_2, references=original_texts)\n",
    "\n",
    "# Print the BLEU score for M2M-100 model\n",
    "print(f\"BLEU Score for Translation Model:\\n{bleu_score}\\n\")\n",
    "\n",
    "# Compute the BLEU score for MBART-50 texts\n",
    "bleu_score = bleu_metric.compute(predictions=translated_texts_3, references=original_texts)\n",
    "\n",
    "# Print the BLEU score for MBART-50 model\n",
    "print(f\"BLEU Score for Translation Model:\\n{bleu_score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQwe28ZS4DSW"
   },
   "source": [
    "### 2) Provide the comparison discussions and analysis based on the evaluation results you obtained from each model. For example, which model performs best, which performs worst, and what is the possible reason for such results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLTwDu8B4DSW"
   },
   "source": [
    "### Based on the BLEU scores provided, the first model performs the best with a BLEU score of 0.4382, followed by the third model with a BLEU score of 0.3961, and the second model performs the worst with a BLEU score of 0.3851.\n",
    "\n",
    "### The BLEU score is a measure of the quality of machine-generated translations, with a higher score indicating a better match with the reference translations. The score ranges from 0 to 1, with 1 being a perfect match.\n",
    "\n",
    "### The first model might have performed the best due to a variety of factors such as the architecture of the model, the size and quality of the training data, and the optimization techniques used during training. The second model might have performed the worst due to limitations in these areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4mL4MuK4DSW"
   },
   "source": [
    "### 3) Select two data samples and compare the translation obtained by the three models with the ground truth. Provide some discussions based on your findings on these two examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "peJPsttC4DSW",
    "outputId": "668fc434-f918-4320-edb3-f8163e2dd46a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: The hearing also marks the date for the suspect’s right to a speedy trial.\n",
      "\n",
      "French Ground truth: L'audience marque également la date pour le droit du suspect à un procès rapide.\n",
      "\n",
      "OPUS-MT Translation: L'audience marque également la date à laquelle le suspect a droit à un procès rapide.\n",
      "\n",
      "M2M-100 Translation: L’audition marque également la date pour le droit du suspect à un procès rapide.\n",
      "\n",
      "MBART-50 Translation: L’audience marque également la date du droit du suspect à un procès rapide.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample 1\n",
    "print(f\"English: {english[0]}\\n\")\n",
    "print(f\"French Ground truth: {original_texts[0]}\\n\")\n",
    "print(f\"OPUS-MT Translation: {translated_texts_1[0]}\\n\")\n",
    "print(f\"M2M-100 Translation: {translated_texts_2[0]}\\n\")\n",
    "print(f\"MBART-50 Translation: {translated_texts_3[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-yK2_c6o4DSW",
    "outputId": "11fbb084-8606-47c3-b9b8-e0790ef1cd1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: Leslie Aun, a spokesperson for the Komen Foundation, said the organization adopted a new rule that does not allow grants or funding to be awarded to organizations that are under legal investigation.\n",
      "\n",
      "French Ground truth: Leslie Aun, porte-parole de la Fondation Komen, a déclaré que l'organisation a adopté une nouvelle règle qui ne permet pas d'accorder de subventions ou de financements aux organisations qui font l'objet d'une enquête judiciaire.\n",
      "\n",
      "OPUS-MT Translation: Leslie Aun, porte-parole de la Fondation Komen, a déclaré que l'organisation a adopté une nouvelle règle qui ne permet pas l'octroi de subventions ou de fonds à des organisations faisant l'objet d'une enquête judiciaire.\n",
      "\n",
      "M2M-100 Translation: Leslie Aun, porte-parole de la Fondation Komen, a déclaré que l'organisation a adopté une nouvelle règle qui ne permet pas que des subventions ou des fonds soient accordés à des organisations qui sont en cours d'enquête juridique.\n",
      "\n",
      "MBART-50 Translation: Leslie Aun, porte-parole de la Fondation Komen, a déclaré que l'organisation avait adopté une nouvelle règle qui ne permet pas que des subventions ou des fonds soient accordés aux organisations faisant l'objet d'une enquête judiciaire.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample 2\n",
    "print(f\"English: {english[1]}\\n\")\n",
    "print(f\"French Ground truth: {original_texts[1]}\\n\")\n",
    "print(f\"OPUS-MT Translation: {translated_texts_1[1]}\\n\")\n",
    "print(f\"M2M-100 Translation: {translated_texts_2[1]}\\n\")\n",
    "print(f\"MBART-50 Translation: {translated_texts_3[1]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9aPEJHY4DSX"
   },
   "source": [
    "### In both samples, all three models (OPUS-MT, M2M-100, and MBART-50) provide accurate translations. However, there are slight differences in phrasing and word choice. The best model depends on the specific requirements of the translation task. For instance, MBART-50 maintains the original sentence structure better in these examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9DOysdf4DSX"
   },
   "source": [
    "#### References:\n",
    "Huggingface model documentation, ChatGPT, Github"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
